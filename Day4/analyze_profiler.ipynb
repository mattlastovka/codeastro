{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Profiling\n",
    "\n",
    "Code profiling is the process of dynamically analyzing the performance of your code or software, focusing on aspects like runtime, memory allocation, and function execution frequency. Its primary goal is to identify performance bottlenecks, excessive resource consumption, and areas that take too long to execute, enabling developers to optimize their code for better efficiency and speed. \n",
    "\n",
    "In this lesson, we only talk about runtime profiling. Use this as an inspiration to check out memory profiling (modules like [`memory_profiler`](https://pypi.org/project/memory-profiler/) and [`tracemalloc`](https://docs.python.org/3/library/tracemalloc.html)).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cProfile (our friendly neighborhood profiler)\n",
    "\n",
    "At its core, a runtime profiler is just a timer (like the `timeit` function). Under the hood, it inserts hooks into the Python interpreter that track when functions start and stop, so it can measure the exact time spent in each one. The result is a detailed performance breakdown of your script. \n",
    "\n",
    "Here we use a simple Python profiler called `cProfile`, which is built into the Python standard library. So, all of you already have it! `cProfile` gets its name from being the C-extension version of the older Python `profile` module. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running cProfile\n",
    "\n",
    "First, create a Python script that repeatedly calls the function or class you want to speed up a few hundred times atleast (this helps average out any variations in execution time due to different input parameters or simply makes sure the most time consuming functions are reflected as such in the output file). Then, run the following command from the command line:\n",
    "\n",
    "`python -m cProfile -o profiler_output.txt profile_orbitize.py`\n",
    "\n",
    "- The `-m cProfile` will attach the cProfile profiler to this script when it runs.\n",
    "\n",
    "- The `-o profiler_output.txt` will save its output to \"profiler_output.txt\" (otherwise it dumps the output stats into stdout).\n",
    "\n",
    "_Note on parallelization: Using parallelism will make the cprofiler output more confusing than necessary. We just want to know what takes the most time,\n",
    "summed up across multiple processes. Doing this on parallelized code, each process must save its own cProfile output and you \n",
    "need to combine the outputs afterwards (using `pstats.add()`)._"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the cProfile output\n",
    "\n",
    "After you have run the `profile_orbitize.py` function with the cProfile package, we will use the `pstats` module (part of Python standard library) to read and analyze the output of the profiler (a binary file) and identify bottlenecks in the code. Reference: [pstats documentation](https://docs.python.org/3/library/profile.html)\n",
    "\n",
    "PS: If you are in Colaboratory, you will need to upload the output of the cProfile to Colaboratory!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pstats\n",
    "\n",
    "mystats = pstats.Stats(\"profiler_output.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the top 5 functions that took the longest time to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below sorts the functions by which took the longest to run, and prints out only the stats of the top 5 functions that took longest to run. The columns below mean:\n",
    "  * \"ncalls\" - number of times this function was called\n",
    "  * \"tottime\" - the total execution time spent in this code NOT including calls to other functions\n",
    "  * \"percall\" - this first percall divides tottime by ncalls. The amount of time per call spent solely in this function.\n",
    "  * \"cumtime\" - the total execution time spent in this code INCLUDING calls to sub functions.\n",
    "  * \"percall\" - second percall divides cumtime by ncalls\n",
    "  * \"filename:lineno(function)\" - shows where the function is defined in the package; its file, line number, and name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mystats.sort_stats(\"time\").print_stats(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which functions to target?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even in such a short script, python called millions of function! A lot of them are super quick and really not what we are after. Typically, we want to find the few functions that are bottlenecks in our code. For this, both `tottime` and `cumtime` are useful. Note: <u>cumtime > tottime always</u>\n",
    "\n",
    "A function with a high `tottime` means we should focus on speeding up this function. A function with only a high `cumtime` means we should see what this function is calling to improve runtime. For example, amongst the functions displayed above:\n",
    "  * `_newton_solver_wrapper()` is a function with a high tottime. We should look at the lines in that function to check out how to speed it up.\n",
    "  * `_calc_ecc_anom()` is a function with a low tottime but a very high `cumtime`. We should look at the functions it calls to figure out what takes up so much time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Filters (Order Matters!)\n",
    "\n",
    "Generally, we want to put some filters on `print_stats`, because otherwise there will be so much printed out it is unmanageable. \n",
    "\n",
    "- For example, calling `sort_stats(\"time\").print_stats(15,\"numpy\")` would pick the top 15 longest runtime functions, and then downselects those which have `numpy` name in them. That would give us less than 15 numpy functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystats.sort_stats(\"time\").print_stats(15, \"numpy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Whereas, the `print_stats(\"numpy\", 15)` command first selects every function with `numpy` in the name and then picks the top 15 that took the most time to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mystats.sort_stats(\"time\").print_stats(\"numpy\", 15)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activities\n",
    "\n",
    "## Activity Part 1: Analyze the output\n",
    "\n",
    "Generate the output above on your own by running `python -m cProfile -o profiler_output.txt profile_orbitize.py` and answer the following questions by analyzing the output.\n",
    "\n",
    "1. Which function takes up the most runtime not including calls to sub-functions?\n",
    "\n",
    "2. Which function takes up the most runtime including calls to sub-functions?\n",
    "\n",
    "3. Which function is called the most? Which `orbitize` function is called the most?\n",
    "\n",
    "4. If we had the magical ability to speed up one of the functions in the previous three answers by a factor of 2, which function should we speed up? What is the improvement in end-to-end runtime of the script?\n",
    "\n",
    "\n",
    "_Note: Depending on your system, some inbuilt functions might turn out to be the most time consuming ones. But here we are looking for the ones we can speed up, namely the `orbitize` functions._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity Part 2: Investigate why `_logl` takes so long\n",
    "\n",
    "`_logl()` is a helper function in `orbitize!` to compute the log likelihood of the data given the model. The `_logl()` function itself has a short runtime but it is calling something that takes a long time that makes it have a long `cumtime`. We can use the `print_callees()` function to look at the stats of all the functions it calls.\n",
    "\n",
    "_Hint: We can see that `compute_model` is the function with the highest cumtime, but its tottime is low, so we know something it calls takes all the time. We must dig deeper! Keep digging down recursively to find what function called within `_logl()` that takes the longest._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Step 1:\n",
    "mystats.print_callees(\"_calc_ecc_anom\")\n",
    "\n",
    "#Step 2:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity Part 3: Which function calls `numpy.array` the most?\n",
    "\n",
    "`numpy.array` is a popular function because it gets called anytime a new numpy array gets created. We can use `print_callers()` to see which functions call it to look into potentially reducing the number of array creations to speed up the code. Which function in `orbitize` calls `numpy.array` the most times per function call (not including calls from subfunctions)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mystats.print_callers('numpy.array')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Optimizing Strategy\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "  <img src=\"imgs/code_speedup.png\" width=\"600\">\n",
    "</div>\n",
    "\n",
    "- Generally, it is important to **focus on the 1 or 2 slowest functions**. Speeding up something that takes 50% of the runtime by 2x gives you a 25% overall speedup. But if the function takes only 10%, the same 2x improvement gives just a 5% gain.\n",
    "\n",
    "- For all else, it is generally more important to optimize code readibility over runtime. Code that is more readible is less prone to bugs and easier to maintain. If a piece of code takes only 0.0001% of the runtime, any amount of speed up is not worth making the code hard to read (from a person-hour perspective). Many times, it is difficult to gain more than a factor of 2 speed up in runtime. __Definitely think about the potential gain in speed-up versus how long it takes to write and maintain the code__.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Ways to Speed Up Your Code\n",
    "\n",
    "There's unfortunately not one single method to speed up your code. Once you have identified the critical chunk of code to speed up, here are a few ideas to try:\n",
    "\n",
    "Level 1:\n",
    "- Eliminate computations that are not being used (e.g., computing variables you do not use; processing parts of images that will be discarded). \n",
    "- Use `numpy` functions whenever possible for matrix operations\n",
    "- Avoid copying variables when they do not need to be copied (e.g., if the input is already a numpy.array, you don't need to wrap it with `np.array()` to ensure it's a numpy array). \n",
    "- Avoid using python `for` and `while` loops (see example below)\n",
    "\n",
    "Level 2:\n",
    "- If MKL/BLAS is being run inside of already-parallelized code, disable MKL/BLAS (see Day 2 parallelization materials for more details on MKL/BLAS).\n",
    "- Reduce the number of iterations or increase the tolerance of routines that are unnecessarily precise (e.g., optimizers can run for less iterations; sin and cos can be approximated by taylor expansions). \n",
    "\n",
    "Level 3:\n",
    "- Turn some of your python code into C-code and call it with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here's an example of replacing a for loop with a faster masked operation.\n",
    "This is called VECTORIZATION.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# add 5 to each value in a list that's greater than 6\n",
    "arr_1 = np.asarray([1, 4, 6, 7, 8, 10, 2])\n",
    "arr_2 = np.asarray([1, 4, 6, 7, 8, 10, 2])\n",
    "\n",
    "\n",
    "# first option (much slower for long arrays)\n",
    "for i, elem in enumerate(arr_1):\n",
    "    if elem > 6:\n",
    "        arr_1[i] += 5\n",
    "\n",
    "# second option (faster)\n",
    "mask_arr = arr_2 > 6\n",
    "arr_2[mask_arr] += 5\n",
    "\n",
    "# print resulting arrays for both cases\n",
    "print(arr_1)\n",
    "print(arr_2)\n",
    "\n",
    "\"\"\"\n",
    "Extra Activity: Time these two methods to see the difference in speed. Use the `time` module.\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Reading: Code Performance Extrapolation\n",
    "\n",
    "If your code takes 24 hours to run to completion, or needs to be run on a cluster, it is likely still easier to develop and benchmark it on your laptop. We can use extrapolation to estimate performance. \n",
    "\n",
    "### Runtime\n",
    "\n",
    "Generally algorithm runtimes scale as N^a where a is some positive number greater than 1, and N is the size of your data. (In CS terms, this is often discussed in \"Big O\" notation as  $O(N^a)$ runtime). If your data is too big or takes too long to benchmark on a single machine, try running your code on much smaller data. If you do this for ~3-5 different test data with different N, you can estimate what is the scaling of your code (i.e., what is \"a\"). Once you know the scaling you can use that scaling relation to extrapolate how long it will take on larger datasets. If you can also use more cores, you can divide the final runtime by the number of cores you can use. \n",
    "\n",
    "For example (very idealized), my code takes 1 second on 10x10 data, 100 seconds on 100x100 data, and 400 seconds on 200x200 data (all on a single core). My code scales as (N/10)^2 where N is the size of one dimension of the data. If I want to run it on 10000x10000 data with 100 cores, my code should take (1000^2)/100 = 10,000 seconds to run. In practice there will probably be some deviations to these estimates (due to overheads, CPU usage scheudling, other factors), but this scaling should give you a general sense of how long something will take. \n",
    "\n",
    "### Memory\n",
    "\n",
    "If you are worried that your program will run out of memory, try calculating how much memory your program uses. On modern 64-bit machines, a single number (an integer, float) takes up 8 bytes. If you have an array of 1000x1000 floats, then it will take up 8e6 bytes or 8 MBs. If you have 3-D numerical data with dimensions 1000x1000x1000, then it will take up 8e9 bytes or 8 GBs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codeastro2025env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
